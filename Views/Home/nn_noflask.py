# -*- coding: utf-8 -*-
"""NN_NOFLASK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W8vnGfIZwu7CPEt-ebZQDOwbzpFY5gEG
"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
from torch.optim import AdamW
from torch.utils.data import DataLoader, Dataset
import unittest

# Custom Dataset for Patent Data
class PatentDataset(Dataset):
    def __init__(self, titles, abstracts, tokenizer, max_length=128):
        self.titles = titles
        self.abstracts = abstracts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.titles)

    def __getitem__(self, idx):
        title = self.titles[idx]
        abstract = self.abstracts[idx]
        combined_text = f"Title: {title}\nAbstract: {abstract}"

        # Tokenize the combined text
        encoded = self.tokenizer(
            combined_text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        input_ids = encoded["input_ids"].squeeze()
        attention_mask = encoded["attention_mask"].squeeze()

        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": input_ids}


# Fine-tune the Pre-trained GPT-2 Model
def fine_tune_model(model, dataloader, epochs=3, lr=5e-5, device="cuda"):
    optimizer = AdamW(model.parameters(), lr=lr)
    model.train()
    model.to(device)

    for epoch in range(epochs):
        total_loss = 0
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            # Forward pass
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            loss = outputs.loss
            total_loss += loss.item()

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}")


# Inference with the Fine-Tuned Model
def generate_response(model, tokenizer, prompt, max_new_tokens=50, device="cuda"):
    model.eval()
    model.to(device)

    # Tokenize the input prompt
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        padding=True,
    ).to(device)

    # Generate text
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=max_new_tokens,
            num_beams=5,
            early_stopping=True,
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


# Main Function to Run the Workflow
def main():
    # Configuration
    model_name = "gpt2"
    batch_size = 2
    max_length = 128
    epochs = 1
    learning_rate = 5e-5
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Example Titles and Abstracts (Manually Provided)
    titles = [
        "A system for AI patent analysis",
        "An innovative method for legal document automation"
    ]
    abstracts = [
       "This system provides AI-based solutions for analyzing patents efficiently.",
        "This method automates the processing of legal documents using machine learning."
    ]

    # Load Pre-trained Model and Tokenizer
    print("Loading pre-trained model and tokenizer...")
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)

    # Assign a padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Create Dataset and DataLoader
    print("Preprocessing data for fine-tuning...")
    dataset = PatentDataset(titles, abstracts, tokenizer, max_length)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Fine-Tune the Model
    print("Fine-tuning the model...")
    fine_tune_model(model, dataloader, epochs, learning_rate, device)

    # Interactive Inference with User Input
    print("Enter your prompts to test the fine-tuned model. Type 'exit' to quit.")
    while True:
        user_prompt = input("Enter your prompt: ")
        if user_prompt.lower() == "exit":
            print("Exiting inference.")
            break
        response = generate_response(model, tokenizer, user_prompt, max_new_tokens=50, device=device)
        print(f"Generated Response: {response}")


# Unit Tests for Core Functionality
class TestPatentProgram(unittest.TestCase):
    def setUp(self):
        """Set up a tokenizer and a small sample dataset for testing."""
        self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        self.tokenizer.pad_token = self.tokenizer.eos_token  # Ensure padding token is set
        self.sample_titles = ["Sample Patent Title 1", "Sample Patent Title 2"]
        self.sample_abstracts = ["Sample Abstract 1", "Sample Abstract 2"]

    def test_dataset_creation(self):
        """Test the creation of a dataset from sample data."""
        dataset = PatentDataset(self.sample_titles, self.sample_abstracts, self.tokenizer, max_length=128)
        self.assertEqual(len(dataset), 2, "Dataset length is incorrect.")
        sample = dataset[0]
        self.assertIn("input_ids", sample, "Dataset sample missing input_ids.")
        self.assertIn("attention_mask", sample, "Dataset sample missing attention_mask.")
        self.assertIn("labels", sample, "Dataset sample missing labels.")

    def test_response_generation(self):
        """Test the generation of a response from a model."""
        model = GPT2LMHeadModel.from_pretrained("gpt2")
        prompt = "Title: An innovative method for processing data\nAbstract:"
        response = generate_response(model, self.tokenizer, prompt, max_new_tokens=50, device="cpu")
        self.assertIsInstance(response, str, "Generated response is not a string.")
        self.assertGreater(len(response), 0, "Generated response is empty.")


if __name__ == "__main__":
    import sys

    # Determine if running tests or the main program
    if len(sys.argv) > 1 and sys.argv[1].startswith('test'):
        del sys.argv[1:]  # Prevent unittest from interpreting file paths
        unittest.main()
    else:
        print("\nRunning main program...")
        main()